{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json, collections, itertools, pickle, nltk, numpy, scipy\n",
    "from nltk.util import ngrams\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading raw data from files and converting to json database\n",
    "\n",
    "def ft_load_raw_data_from_fs():\n",
    "    if not os.path.isfile(\"./raw_data.json\"):\n",
    "        dir_list = os.listdir(\"./Dataset/\")\n",
    "        raw_data = {}\n",
    "        for dir_ in dir_list:\n",
    "            raw_data[dir_] = {}\n",
    "            for fl in glob.glob(os.path.join(\"./Dataset/\"+dir_+\"/\",\"*.txt\")):\n",
    "                fp = open(fl)\n",
    "                text = fp.read()\n",
    "                fp.close()\n",
    "                offset = text.index(\"     \")\n",
    "                entry_name = text[:offset]\n",
    "                entry = text[offset+5:].strip(\" \").split()\n",
    "                raw_data[dir_][entry_name]=entry\n",
    "        with open('raw_data.json', 'w', encoding=\"utf-8\") as outfile:\n",
    "            json.dump(raw_data, outfile, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        with open(\"./raw_data.json\") as json_file:\n",
    "                raw_data = json.load(json_file)\n",
    "\n",
    "    return (raw_data)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting count for all assembly instructions from the dataset\n",
    "def ft_get_top_opcodes(raw_data):\n",
    "    count =  {}\n",
    "    total = 0\n",
    "    for rware in raw_data:\n",
    "        for entry in raw_data[rware]:\n",
    "            for word in raw_data[rware][entry]:\n",
    "                if word in count:\n",
    "                    count[word] += 1\n",
    "                else:\n",
    "                    count[word] = 1\n",
    "    count = dict(sorted(count.items(), key = lambda k:k[1], reverse=True))\n",
    "    total = sum(count.values())\n",
    "    # Getting the top instructions to cover 90% of the data\n",
    "\n",
    "    n = 0\n",
    "    top = []\n",
    "\n",
    "    for inst in count:\n",
    "        if n/total >= 0.9:\n",
    "            break\n",
    "        top.append(inst)\n",
    "        n += count[inst]\n",
    "    \n",
    "    return (count, top)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_vectorize(count, top, raw_data):\n",
    "    # creating features based on ngrams\n",
    "\n",
    "    unigrams = list(count.keys())\n",
    "    bigrams = list(itertools.permutations(top, 2))\n",
    "    trigrams = list(itertools.permutations(top, 3))\n",
    "\n",
    "    # creating column labels\n",
    "    columns = unigrams + bigrams + trigrams\n",
    "    columns.append(\"Ransomware Type\")\n",
    "\n",
    "    # vectorizing unigrams\n",
    "\n",
    "    univecs = {}\n",
    "    for rware in raw_data:\n",
    "        for entry in raw_data[rware]:\n",
    "            vector = []\n",
    "            for word in unigrams:\n",
    "                vector.append(raw_data[rware][entry].count(word))\n",
    "            # vector.append(rware)\n",
    "            univecs[entry] = vector\n",
    "\n",
    "    # vectorizing bigrams\n",
    "\n",
    "    bivecs = {}\n",
    "    for rware in raw_data:\n",
    "        for entry in raw_data[rware]:\n",
    "            temp1 = list(ngrams(raw_data[rware][entry],2))\n",
    "            temp2 = dict(collections.Counter(temp1))\n",
    "            vec = dict((x,0) for x in bigrams)\n",
    "            for gram in temp2:\n",
    "                if gram in bigrams:\n",
    "                    vec[gram] = temp2[gram]\n",
    "            bivecs[entry] = list(vec.values())\n",
    "\n",
    "    # vectorizing trigrams\n",
    "\n",
    "    trivecs = {}\n",
    "    for rware in raw_data:\n",
    "        for entry in raw_data[rware]:\n",
    "            temp1 = list(ngrams(raw_data[rware][entry],3))\n",
    "            temp2 = dict(collections.Counter(temp1))\n",
    "            vec = dict((x,0) for x in trigrams)\n",
    "            for gram in temp2:\n",
    "                if gram in trigrams:\n",
    "                    vec[gram] = temp2[gram]\n",
    "            trivecs[entry] = list(vec.values())\n",
    "    \n",
    "    return (columns, univecs, bivecs, trivecs)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_get_vectors():\n",
    "    if os.path.isfile(\"./Data/data_vectors.csv\"):\n",
    "        df= pd.read_csv(\"./Data/data_vectors.csv\")\n",
    "        return (df.drop(df.columns[0],axis=1))\n",
    "    else:\n",
    "        raw_data = ft_load_raw_data_from_fs()\n",
    "        count, top = ft_get_top_opcodes(raw_data)\n",
    "        columns, univecs, bivecs, trivecs = ft_vectorize(count, top, raw_data)\n",
    "\n",
    "        data = {}\n",
    "        for rware in raw_data:\n",
    "            for entry in raw_data[rware]:\n",
    "                temp = [] + univecs[entry] + bivecs[entry] + trivecs[entry] + [rware]\n",
    "                data[entry] = temp\n",
    "\n",
    "        df = pd.DataFrame(list(data.values()), columns=columns)\n",
    "        # replacing ransomware names with integers\n",
    "\n",
    "        df.replace(\"Cerber\",1, inplace=True)\n",
    "        df.replace(\"CryptoWall\",2, inplace=True)\n",
    "        df.replace(\"CTB-Locker\",3, inplace=True)\n",
    "        df.replace(\"Locky\",4, inplace=True)\n",
    "        df.replace(\"Sage\",5, inplace=True)\n",
    "        df.replace(\"TeslaCrypt\",6, inplace=True)\n",
    "        map_ = {1:\"Cerber\",2:\"CryptoWall\",3:\"CTB-Locker\",4:\"Locky\",5:\"Sage\",6:\"TeslaCrypt\"}\n",
    "\n",
    "        # dropping column where all counts are 0\n",
    "        df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "        df.to_csv(\"./Data/data_vectors.csv\")\n",
    "        with open('Results/top-opcodes.txt','w') as fp:\n",
    "            fp.write(str(top))\n",
    "\n",
    "        return (df)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data in the form of count vectors\n",
    "map_ = {1:\"Cerber\",2:\"CryptoWall\",3:\"CTB-Locker\",4:\"Locky\",5:\"Sage\",6:\"TeslaCrypt\"}\n",
    "df = ft_get_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataframe into test and train sets 60-40 ratio using stratified sampling\n",
    "y = df.pop('Ransomware Type')\n",
    "X_train_counts, X_test_counts, Y_train_counts, Y_test_counts = skl.model_selection.train_test_split(df, y, test_size=0.33, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting probabilities based on all of test data\n",
    "X_train_norm_all = X_train_counts/X_train_counts.sum()\n",
    "X_train_norm_all.fillna(0, inplace = True)\n",
    "X_test_norm_all = X_test_counts\n",
    "Y_train_norm_all = Y_train_counts\n",
    "Y_test_norm_all = Y_test_counts"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting metrics for features as counts\n",
    "\n",
    "\n",
    "# classification labels\n",
    "metrics = [['Classifier\\t', 'Precision\\t', 'Recall\\t\\t', 'F-Score']]\n",
    "\n",
    "# logistic regression on counts\n",
    "lr_counts = skl.linear_model.LogisticRegression(penalty = 'l2',solver = 'newton-cg', fit_intercept = True).fit(X_train_counts,Y_train_counts)\n",
    "# lr_counts_acc = skl.metrics.accuracy_score(Y_test_counts,lr_counts.predict(X_test_counts))\n",
    "plot_cm_lr_counts = skl.metrics.plot_confusion_matrix(lr_counts, X_test_counts,Y_test_counts, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_lr_counts.figure_.savefig('Figures/LR-Counts-CM.png')\n",
    "prf_lr_counts = skl.metrics.precision_recall_fscore_support(Y_test_counts,lr_counts.predict(X_test_counts), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['Logistic Regression'] + list(prf_lr_counts[:-1]))\n",
    "\n",
    "# svm on counts\n",
    "svm_counts = skl.svm.LinearSVC(penalty = 'l2').fit(X_train_counts,Y_train_counts)\n",
    "svm_counts_acc = skl.metrics.accuracy_score(Y_test_counts,svm_counts.predict(X_test_counts))\n",
    "plot_cm_svm_counts = skl.metrics.plot_confusion_matrix(svm_counts, X_test_counts,Y_test_counts, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_svm_counts.figure_.savefig('Figures/SVM-Counts-CM.png')\n",
    "prf_svm_counts = skl.metrics.precision_recall_fscore_support(Y_test_counts,svm_counts.predict(X_test_counts), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['SVM\\t\\t'] + list(prf_svm_counts[:-1]))\n",
    "\n",
    "\n",
    "# random forest classifier on counts\n",
    "rf_counts = skl.ensemble.RandomForestClassifier(criterion='gini', n_estimators = 100).fit(X_train_counts,Y_train_counts)\n",
    "rf_counts_acc = skl.metrics.accuracy_score(Y_test_counts,rf_counts.predict(X_test_counts))\n",
    "plot_cm_rf_counts = skl.metrics.plot_confusion_matrix(rf_counts, X_test_counts,Y_test_counts, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_rf_counts.figure_.savefig('Figures/RF-Counts-CM.png')\n",
    "prf_rf_counts = skl.metrics.precision_recall_fscore_support(Y_test_counts,rf_counts.predict(X_test_counts), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['Random Forest\\t'] + list(prf_rf_counts[:-1]))\n",
    "\n",
    "\n",
    "# decision tree for counts\n",
    "dtree_counts = skl.tree.DecisionTreeClassifier(criterion='gini').fit(X_train_counts,Y_train_counts)\n",
    "dtree_counts_acc = skl.metrics.accuracy_score(Y_test_counts,dtree_counts.predict(X_test_counts))\n",
    "plot_cm_dtree_counts = skl.metrics.plot_confusion_matrix(dtree_counts, X_test_counts,Y_test_counts, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_dtree_counts.figure_.savefig('Figures/dtree-Counts-CM.png')\n",
    "prf_dtree_counts = skl.metrics.precision_recall_fscore_support(Y_test_counts,dtree_counts.predict(X_test_counts), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['Decision Tree\\t'] + list(prf_lr_counts[:-1]))\n",
    "\n",
    "\n",
    "# knn classifier on counts\n",
    "knn_counts = skl.neighbors.KNeighborsClassifier(algorithm = 'auto', metric = 'l1', n_neighbors = 3).fit(X_train_counts,Y_train_counts)\n",
    "knn_counts_acc = skl.metrics.accuracy_score(Y_test_counts,knn_counts.predict(X_test_counts))\n",
    "plot_cm_knn_counts = skl.metrics.plot_confusion_matrix(knn_counts, X_test_counts,Y_test_counts, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_knn_counts.figure_.savefig('Figures/KNN-Counts-CM.png')\n",
    "prf_knn_counts = skl.metrics.precision_recall_fscore_support(Y_test_counts,knn_counts.predict(X_test_counts), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['KNN\\t\\t'] + list(prf_knn_counts[:-1]))\n",
    "\n",
    "op_counts = ''\n",
    "for row in metrics:\n",
    "    line = ''\n",
    "    for col in row:\n",
    "        if type(col) == float:\n",
    "            line += '{:.4f}\\t'.format(col)\n",
    "        else:\n",
    "            line += '{}\\t'.format(col)\n",
    "    op_counts += line + '\\n'\n",
    "\n",
    "with open('Results/Count_metrics.txt','w') as fp:\n",
    "    fp.write(op_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting metrics for features as probabilities\n",
    "\n",
    "# classification labels\n",
    "metrics = [['Classifier\\t', 'Precision\\t', 'Recall\\t\\t', 'F-Score']]\n",
    "\n",
    "\n",
    "# logistic regression on normalized probabilities\n",
    "lr_norm_all = skl.linear_model.LogisticRegression(penalty = 'none',solver = 'newton-cg', fit_intercept = True).fit(X_train_norm_all,Y_train_norm_all)\n",
    "lr_norm_all_acc = skl.metrics.accuracy_score(Y_test_norm_all,lr_norm_all.predict(X_test_norm_all))\n",
    "plot_cm_lr_norm_all = skl.metrics.plot_confusion_matrix(lr_norm_all, X_test_norm_all,Y_test_norm_all, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_lr_norm_all.figure_.savefig('Figures/LR-Normalized-CM.png')\n",
    "prf_lr_norm_all = skl.metrics.precision_recall_fscore_support(Y_test_norm_all,lr_norm_all.predict(X_test_norm_all), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['Logistic Regression'] + list(prf_lr_norm_all[:-1]))\n",
    "\n",
    "# svm on counts\n",
    "svm_norm_all = skl.svm.LinearSVC().fit(X_train_norm_all,Y_train_norm_all)\n",
    "svm_norm_all_acc = skl.metrics.accuracy_score(Y_test_norm_all,svm_norm_all.predict(X_test_norm_all))\n",
    "plot_cm_svm_norm_all = skl.metrics.plot_confusion_matrix(svm_norm_all, X_test_norm_all,Y_test_norm_all, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_svm_norm_all.figure_.savefig('Figures/SVM-Normalized-CM.png')\n",
    "prf_svm_norm_all = skl.metrics.precision_recall_fscore_support(Y_test_norm_all,svm_norm_all.predict(X_test_norm_all), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['SVM\\t\\t'] + list(prf_svm_norm_all[:-1]))\n",
    "\n",
    "\n",
    "# random forest classifier on counts\n",
    "rf_norm_all = skl.ensemble.RandomForestClassifier(criterion='gini', n_estimators = 100).fit(X_train_norm_all,Y_train_norm_all)\n",
    "rf_norm_all_acc = skl.metrics.accuracy_score(Y_test_norm_all,rf_norm_all.predict(X_test_norm_all))\n",
    "plot_cm_rf_norm_all = skl.metrics.plot_confusion_matrix(rf_norm_all, X_test_norm_all,Y_test_norm_all, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_rf_norm_all.figure_.savefig('Figures/RF-Normalized-CM.png')\n",
    "prf_rf_norm_all = skl.metrics.precision_recall_fscore_support(Y_test_norm_all,rf_norm_all.predict(X_test_norm_all), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['Random Forest\\t'] + list(prf_rf_norm_all[:-1]))\n",
    "\n",
    "\n",
    "# # decision tree for counts\n",
    "dtree_norm_all = skl.tree.DecisionTreeClassifier(criterion='gini').fit(X_train_norm_all,Y_train_norm_all)\n",
    "dtree_norm_all_acc = skl.metrics.accuracy_score(Y_test_norm_all,dtree_norm_all.predict(X_test_norm_all))\n",
    "plot_cm_dtree_norm_all = skl.metrics.plot_confusion_matrix(dtree_norm_all, X_test_norm_all,Y_test_norm_all, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_dtree_norm_all.figure_.savefig('Figures/dtree-Normalized-CM.png')\n",
    "prf_dtree_norm_all = skl.metrics.precision_recall_fscore_support(Y_test_norm_all,dtree_norm_all.predict(X_test_norm_all), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['Decision Tree\\t'] + list(prf_lr_norm_all[:-1]))\n",
    "\n",
    "\n",
    "# # knn classifier on counts\n",
    "knn_norm_all = skl.neighbors.KNeighborsClassifier(algorithm = 'auto', metric = scipy.spatial.distance.jensenshannon, n_neighbors = 3).fit(X_train_norm_all,Y_train_norm_all)\n",
    "knn_norm_all_acc = skl.metrics.accuracy_score(Y_test_norm_all,knn_norm_all.predict(X_test_norm_all))\n",
    "plot_cm_knn_norm_all = skl.metrics.plot_confusion_matrix(knn_norm_all, X_test_norm_all,Y_test_norm_all, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_knn_norm_all.figure_.savefig('Figures/KNN-Normalized-CM.png')\n",
    "prf_knn_norm_all = skl.metrics.precision_recall_fscore_support(Y_test_norm_all,knn_norm_all.predict(X_test_norm_all), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['KNN\\t\\t'] + list(prf_knn_norm_all[:-1]))\n",
    "\n",
    "                                             \n",
    "op_counts = ''\n",
    "for row in metrics:\n",
    "    line = ''\n",
    "    for col in row:\n",
    "        if type(col) == float:\n",
    "            line += '{:.4f}\\t'.format(col)\n",
    "        else:\n",
    "            line += '{}\\t'.format(col)\n",
    "    op_counts += line + '\\n'\n",
    "\n",
    "with open('Results/Normalized_metrics.txt','w') as fp:\n",
    "    fp.write(op_counts)\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training lasso regression model to obtain weights for columns\n",
    "# dropping features if lasso weights are zero\n",
    "# saving selected features to file\n",
    "\n",
    "lasso = skl.linear_model.Lasso(max_iter = 10000).fit(X_train_counts,Y_train_counts)\n",
    "lasso_coeff = lasso.coef_.tolist()\n",
    "drop_zero = [i for i in range(len(lasso_coeff)) if lasso_coeff[i] == 0]\n",
    "selected_features = X_train_counts.columns.to_list()\n",
    "with open('Results/selected_features.txt','w') as fp:\n",
    "    fp.write(str(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts = X_train_counts.drop([X_train_counts.columns[i] for i in drop_zero],axis=1)\n",
    "X_test_counts = X_test_counts.drop([X_test_counts.columns[i] for i in drop_zero],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting metrics for selected features as counts\n",
    "\n",
    "\n",
    "# classification labels\n",
    "metrics = [['Classifier\\t', 'Precision\\t', 'Recall\\t\\t', 'F-Score']]\n",
    "\n",
    "# logistic regression on counts\n",
    "lr_counts = skl.linear_model.LogisticRegression(penalty = 'l2',solver = 'newton-cg', fit_intercept = True).fit(X_train_counts,Y_train_counts)\n",
    "# lr_counts_acc = skl.metrics.accuracy_score(Y_test_counts,lr_counts.predict(X_test_counts))\n",
    "plot_cm_lr_counts = skl.metrics.plot_confusion_matrix(lr_counts, X_test_counts,Y_test_counts, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_lr_counts.figure_.savefig('Figures/LR-Counts-selected-CM.png')\n",
    "prf_lr_counts = skl.metrics.precision_recall_fscore_support(Y_test_counts,lr_counts.predict(X_test_counts), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['Logistic Regression'] + list(prf_lr_counts[:-1]))\n",
    "\n",
    "# svm on counts\n",
    "svm_counts = skl.svm.LinearSVC(penalty = 'l2').fit(X_train_counts,Y_train_counts)\n",
    "svm_counts_acc = skl.metrics.accuracy_score(Y_test_counts,svm_counts.predict(X_test_counts))\n",
    "plot_cm_svm_counts = skl.metrics.plot_confusion_matrix(svm_counts, X_test_counts,Y_test_counts, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_svm_counts.figure_.savefig('Figures/SVM-Counts-selected-CM.png')\n",
    "prf_svm_counts = skl.metrics.precision_recall_fscore_support(Y_test_counts,svm_counts.predict(X_test_counts), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['SVM\\t\\t'] + list(prf_svm_counts[:-1]))\n",
    "\n",
    "\n",
    "# random forest classifier on counts\n",
    "rf_counts = skl.ensemble.RandomForestClassifier(criterion='gini', n_estimators = 100).fit(X_train_counts,Y_train_counts)\n",
    "rf_counts_acc = skl.metrics.accuracy_score(Y_test_counts,rf_counts.predict(X_test_counts))\n",
    "plot_cm_rf_counts = skl.metrics.plot_confusion_matrix(rf_counts, X_test_counts,Y_test_counts, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_rf_counts.figure_.savefig('Figures/RF-Counts-selected-CM.png')\n",
    "prf_rf_counts = skl.metrics.precision_recall_fscore_support(Y_test_counts,rf_counts.predict(X_test_counts), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['Random Forest\\t'] + list(prf_rf_counts[:-1]))\n",
    "\n",
    "\n",
    "# decision tree for counts\n",
    "dtree_counts = skl.tree.DecisionTreeClassifier(criterion='gini').fit(X_train_counts,Y_train_counts)\n",
    "dtree_counts_acc = skl.metrics.accuracy_score(Y_test_counts,dtree_counts.predict(X_test_counts))\n",
    "plot_cm_dtree_counts = skl.metrics.plot_confusion_matrix(dtree_counts, X_test_counts,Y_test_counts, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_dtree_counts.figure_.savefig('Figures/dtree-Counts-selected-CM.png')\n",
    "prf_dtree_counts = skl.metrics.precision_recall_fscore_support(Y_test_counts,dtree_counts.predict(X_test_counts), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['Decision Tree\\t'] + list(prf_lr_counts[:-1]))\n",
    "\n",
    "\n",
    "# knn classifier on counts\n",
    "knn_counts = skl.neighbors.KNeighborsClassifier(algorithm = 'auto', metric = 'l1', n_neighbors = 3).fit(X_train_counts,Y_train_counts)\n",
    "knn_counts_acc = skl.metrics.accuracy_score(Y_test_counts,knn_counts.predict(X_test_counts))\n",
    "plot_cm_knn_counts = skl.metrics.plot_confusion_matrix(knn_counts, X_test_counts,Y_test_counts, display_labels=[_ for _ in map_], cmap = plt.cm.BuGn)\n",
    "plot_cm_knn_counts.figure_.savefig('Figures/KNN-Counts-selected-CM.png')\n",
    "prf_knn_counts = skl.metrics.precision_recall_fscore_support(Y_test_counts,knn_counts.predict(X_test_counts), labels=[1,2,3,4,5,6], average = 'weighted')\n",
    "metrics.append(['KNN\\t\\t'] + list(prf_knn_counts[:-1]))\n",
    "\n",
    "op_counts = ''\n",
    "for row in metrics:\n",
    "    line = ''\n",
    "    for col in row:\n",
    "        if type(col) == float:\n",
    "            line += '{:.4f}\\t'.format(col)\n",
    "        else:\n",
    "            line += '{}\\t'.format(col)\n",
    "    op_counts += line + '\\n'\n",
    "\n",
    "with open('Results/Count_selected_metrics.txt','w') as fp:\n",
    "    fp.write(op_counts)\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}